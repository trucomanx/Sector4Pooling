{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f294d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b0f452-3278-49c0-9601-c8a443dbc32b",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83c3fa6-dbbb-4644-a12f-e497f971a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seed for the random variables\n",
    "SEED_NUMBER=0;\n",
    "\n",
    "## Kfold \n",
    "K=5;                    # Variable K of kfold\n",
    "FOLD_STATUS_FILE='fold_status.json';\n",
    "\n",
    "## Training hyperparameters\n",
    "EPOCHS=80;\n",
    "BATCH_SIZE=32;\n",
    "\n",
    "## Model of network\n",
    "#MODEL_TYPE  = 'model_sector4';\n",
    "MODEL_TYPE = 'model_max'\n",
    "\n",
    "#DATASET_NAME = 'mcfer2023';\n",
    "DATASET_NAME = 'ck+48';\n",
    "\n",
    "OUTPUT_BASE_DIR='output';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c76677-df3b-4ab9-aebd-73ec0245038b",
   "metadata": {},
   "source": [
    "# Bibliotecas externas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3410f40c-893b-449d-b024-7781e0d76733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando/.local/lib/python3.8/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/home/fernando/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60e27cb9-4b9a-4bec-bd30-d9b7a23ec5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e80a0-2ad3-4a43-9a2d-affaa523530a",
   "metadata": {},
   "source": [
    "# Biblioteca local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ce46fc-8392-4e0b-a820-3ce426fcc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('library');\n",
    "sys.path.append('../src');\n",
    "\n",
    "import tools_dataset as toolsd\n",
    "import tools_model   as toolsm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab5c325-9eda-4f6b-b4d8-99d2d433d896",
   "metadata": {},
   "source": [
    "# If command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32302454-de63-4f6c-be9f-a72aac1643d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     MODEL_TYPE: model_max\n",
      "         EPOCHS: 80\n",
      "     BATCH_SIZE: 32\n",
      "   DATASET_NAME: ck+48\n",
      "OUTPUT_BASE_DIR: output\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(sys.argv)):\n",
    "    if sys.argv[n]=='--model':\n",
    "        MODEL_TYPE=sys.argv[n+1];\n",
    "    if sys.argv[n]=='--epochs':\n",
    "        EPOCHS=int(sys.argv[n+1]);\n",
    "    if sys.argv[n]=='--batch-size':\n",
    "        BATCH_SIZE=int(sys.argv[n+1]);\n",
    "    if sys.argv[n]=='--dataset':\n",
    "        DATASET_NAME=sys.argv[n+1];\n",
    "    if sys.argv[n]=='--output-base-dir':\n",
    "        OUTPUT_BASE_DIR=sys.argv[n+1];\n",
    "        \n",
    "print('     MODEL_TYPE:',MODEL_TYPE)\n",
    "print('         EPOCHS:',EPOCHS)\n",
    "print('     BATCH_SIZE:',BATCH_SIZE)\n",
    "print('   DATASET_NAME:',DATASET_NAME)\n",
    "print('OUTPUT_BASE_DIR:',OUTPUT_BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbf13b-1dbe-4a62-bcd2-7347a8c219c5",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f213c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_csv_train_file: /mnt/boveda/DATASETs/FACE-EMOTION/CK+48/CK+48/labels.csv\n",
      " dataset_csv_test_file: /mnt/boveda/DATASETs/FACE-EMOTION/CK+48/CK+48/labels.csv\n"
     ]
    }
   ],
   "source": [
    "dataset_csv_train_file, dataset_csv_test_file, dataset_train_base_dir, input_shape, nout = toolsd.load_dataset(dataset_name=DATASET_NAME);\n",
    "\n",
    "print('dataset_csv_train_file:',dataset_csv_train_file)\n",
    "print(' dataset_csv_test_file:',dataset_csv_test_file)\n",
    "\n",
    "OUTPUT_BASE_DIR=os.path.join(OUTPUT_BASE_DIR,DATASET_NAME);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d1dee5-778a-4fd9-80de-90620bb33128",
   "metadata": {},
   "source": [
    "# Set seed of random variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddef12f-6604-4c71-9473-15f328e954dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED_NUMBER)\n",
    "tf.keras.utils.set_random_seed(int(SEED_NUMBER));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da712f-5937-4965-9623-82c4ee139297",
   "metadata": {},
   "source": [
    "# Setting the cross-validation kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcab2262-3467-4ee3-86a8-9eda4f5ff726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      output_dir: output/skfold5_model_max\n",
      "fold_status_path: output/skfold5_model_max/fold_status.json\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# output directory\n",
    "output_dir = os.path.join(OUTPUT_BASE_DIR,'skfold'+str(K)+'_'+MODEL_TYPE);\n",
    "\n",
    "# K-fold object\n",
    "kf = StratifiedKFold(n_splits = K, shuffle = True, random_state = SEED_NUMBER);\n",
    "\n",
    "# fold status file\n",
    "fold_status_path=os.path.join(output_dir,FOLD_STATUS_FILE);\n",
    "\n",
    "print('      output_dir:',output_dir)\n",
    "print('fold_status_path:',fold_status_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a59004-739e-4760-bd91-ee3808207e3e",
   "metadata": {},
   "source": [
    "# Loading data of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1abe0e9b-ee46-40d8-97c3-12a1efc0cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           filename     label\n",
      "0       anger/S010_004_00000017.png     anger\n",
      "1     sadness/S011_002_00000020.png   sadness\n",
      "2        fear/S011_003_00000012.png      fear\n",
      "3    surprise/S010_002_00000012.png  surprise\n",
      "4       happy/S010_006_00000013.png     happy\n",
      "..                              ...       ...\n",
      "745  surprise/S137_001_00000013.png  surprise\n",
      "746  surprise/S137_001_00000014.png  surprise\n",
      "747  surprise/S138_004_00000011.png  surprise\n",
      "748  surprise/S138_004_00000012.png  surprise\n",
      "749  surprise/S138_004_00000013.png  surprise\n",
      "\n",
      "[750 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load filenames and labels\n",
    "train_data = pd.read_csv(dataset_csv_train_file);\n",
    "print(train_data)\n",
    "\n",
    "# Setting labels\n",
    "Y   = train_data[['label']];\n",
    "\n",
    "L=np.shape(Y)[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67beeb38-1ad1-455d-8a71-5900a1e88163",
   "metadata": {},
   "source": [
    "# Data augmentation configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2e3e68f-50e7-4f8c-9cd6-ae3261e8f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "idg    = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range = 10,\n",
    "                            width_shift_range= 0.07,\n",
    "                            height_shift_range= 0.07,\n",
    "                            horizontal_flip=True,\n",
    "                            shear_range=1.25,\n",
    "                            zoom_range = [float(0.9), float(1.1)] \n",
    "                            )\n",
    "\n",
    "idg_val= ImageDataGenerator(rescale=1./255 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757c163-8553-4377-a387-a9cd3131ac04",
   "metadata": {},
   "source": [
    "# Auxiliar function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8e6ca47-f504-47c5-99ad-db41692d40d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5189cf-b447-4b0a-b9f3-56f304d6fdde",
   "metadata": {},
   "source": [
    "# Creating output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a786de7-43ac-4597-92eb-d9eee66f81d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try: \n",
    "    os.makedirs(output_dir) \n",
    "except: \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf04e51-7a7b-4116-8e1e-04e5f358c5dc",
   "metadata": {},
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13cd68b8-e1aa-4ef4-af04-e05781f309ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 of 5\n",
      "length train: 600 elements\n",
      "length val  : 150 elements\n",
      "Loaded model_max\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 96, 96, 16)        5824      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 96, 96, 4)         5188      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 48, 48, 4)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 48, 48, 16)        5200      \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 48, 48, 4)         3140      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 24, 24, 4)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 16)        3152      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 4)         1604      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 4)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 12, 12, 16)        1616      \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 12, 12, 4)         580       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 6, 6, 4)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 6, 6, 16)          592       \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 6, 6, 4)           580       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 3, 3, 4)          0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 3, 3, 16)          592       \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 4)           580       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 36)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                592       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,325\n",
      "Trainable params: 29,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 600 validated image filenames belonging to 5 classes.\n",
      "Found 150 validated image filenames belonging to 5 classes.\n",
      "Epoch 1/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.5516 - categorical_accuracy: 0.3133\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.33333, saving model to output/skfold5_model_max/model_1.h5\n",
      "19/19 [==============================] - 35s 2s/step - loss: 1.5516 - categorical_accuracy: 0.3133 - val_loss: 1.5477 - val_categorical_accuracy: 0.3333\n",
      "Epoch 2/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.5109 - categorical_accuracy: 0.3400\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.33333\n",
      "19/19 [==============================] - 34s 2s/step - loss: 1.5109 - categorical_accuracy: 0.3400 - val_loss: 1.4947 - val_categorical_accuracy: 0.3333\n",
      "Epoch 3/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.4702 - categorical_accuracy: 0.4317\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.33333\n",
      "19/19 [==============================] - 35s 2s/step - loss: 1.4702 - categorical_accuracy: 0.4317 - val_loss: 1.3752 - val_categorical_accuracy: 0.3333\n",
      "Epoch 4/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.2029 - categorical_accuracy: 0.5150\n",
      "Epoch 4: val_categorical_accuracy improved from 0.33333 to 0.62667, saving model to output/skfold5_model_max/model_1.h5\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.2029 - categorical_accuracy: 0.5150 - val_loss: 1.0153 - val_categorical_accuracy: 0.6267\n",
      "Epoch 5/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.1013 - categorical_accuracy: 0.6033\n",
      "Epoch 5: val_categorical_accuracy improved from 0.62667 to 0.73333, saving model to output/skfold5_model_max/model_1.h5\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.1013 - categorical_accuracy: 0.6033 - val_loss: 0.8261 - val_categorical_accuracy: 0.7333\n",
      "Epoch 6/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.0225 - categorical_accuracy: 0.6067\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.73333\n",
      "19/19 [==============================] - 31s 2s/step - loss: 1.0225 - categorical_accuracy: 0.6067 - val_loss: 0.8523 - val_categorical_accuracy: 0.6400\n",
      "Epoch 7/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.8951 - categorical_accuracy: 0.6750\n",
      "Epoch 7: val_categorical_accuracy improved from 0.73333 to 0.76667, saving model to output/skfold5_model_max/model_1.h5\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.8951 - categorical_accuracy: 0.6750 - val_loss: 0.6889 - val_categorical_accuracy: 0.7667\n",
      "Epoch 8/80\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7853 - categorical_accuracy: 0.7167\n",
      "Epoch 8: val_categorical_accuracy improved from 0.76667 to 0.80000, saving model to output/skfold5_model_max/model_1.h5\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.7853 - categorical_accuracy: 0.7167 - val_loss: 0.6356 - val_categorical_accuracy: 0.8000\n",
      "Epoch 9/80\n",
      " 4/19 [=====>........................] - ETA: 23s - loss: 0.9214 - categorical_accuracy: 0.6250"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e0b97429abba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# This saves the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# FIT THE MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     history = model.fit(train_data_generator,\n\u001b[0m\u001b[1;32m     82\u001b[0m                         \u001b[0;31m#steps_per_epoch=STEPS_BY_EPOCHS,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msrc/cysignals/signals.pyx\u001b[0m in \u001b[0;36mcysignals.signals.python_check_interrupt\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tools_model as mpp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "list_train_index=[];\n",
    "list_val_index=[];\n",
    "for train_index, val_index in kf.split(np.zeros(L),Y):\n",
    "    list_train_index.append(train_index);\n",
    "    list_val_index.append(val_index);\n",
    "\n",
    "import json\n",
    "data_fold =  {'VALIDATION_ACCURACY': [],'VALIDATION_LOSS': [] };\n",
    "\n",
    "fold_var=1;\n",
    "\n",
    "if os.path.isfile(fold_status_path):\n",
    "    # Read JSON file\n",
    "    with open(fold_status_path) as data_file:\n",
    "        data_fold = json.load(data_file)\n",
    "        fold_var=len(data_fold['VALIDATION_ACCURACY'])+1;\n",
    "    \n",
    "while fold_var<=K:\n",
    "    training_data   = train_data.iloc[list_train_index[fold_var-1]]\n",
    "    validation_data = train_data.iloc[list_val_index[fold_var-1]]\n",
    "\n",
    "    print('\\nFold',fold_var,'of',K);\n",
    "    print('length train:',len(list_train_index[fold_var-1]),'elements');\n",
    "    print('length val  :',len(list_val_index[fold_var-1]),'elements');\n",
    "\n",
    "    # CREATE NEW MODEL\n",
    "    \n",
    "    model, target_size = mpp.create_model(  file_of_weight='',\n",
    "                                            model_type=MODEL_TYPE,\n",
    "                                            input_shape=input_shape,\n",
    "                                            nout=nout)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    train_data_generator = idg.flow_from_dataframe(training_data, \n",
    "                                                   directory = dataset_train_base_dir,\n",
    "                                                   target_size=target_size,\n",
    "                                                   x_col = \"filename\", \n",
    "                                                   y_col = \"label\",\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   class_mode=\"categorical\",\n",
    "                                                   shuffle = True);\n",
    "    \n",
    "    valid_data_generator  = idg_val.flow_from_dataframe(validation_data, \n",
    "                                                    directory = dataset_train_base_dir,\n",
    "                                                    target_size=target_size,\n",
    "                                                    x_col = \"filename\", \n",
    "                                                    y_col = \"label\",\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode=\"categorical\",\n",
    "                                                    shuffle = True)\n",
    "    \n",
    "    #STEPS_BY_EPOCHS=len(train_data_generator);\n",
    "    \n",
    "\n",
    "    \n",
    "    # COMPILE NEW MODEL\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    \n",
    "    # CREATE CALLBACKS\n",
    "    best_model_file=os.path.join(output_dir,get_model_name(fold_var));\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=best_model_file, \n",
    "                                                    save_weights_only=True,\n",
    "                                                    monitor='val_categorical_accuracy', \n",
    "                                                    save_best_only=True, \n",
    "                                                    verbose=1);\n",
    "    \n",
    "    log_dir = os.path.join(output_dir,\"logs\",\"fit\" , datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    \n",
    "    # There can be other callbacks, but just showing one because it involves the model name\n",
    "    # This saves the best model\n",
    "    # FIT THE MODEL\n",
    "    history = model.fit(train_data_generator,\n",
    "                        #steps_per_epoch=STEPS_BY_EPOCHS,\n",
    "                        epochs=EPOCHS,\n",
    "                        validation_data=valid_data_generator,\n",
    "                        callbacks=[checkpoint,tensorboard_callback],\n",
    "                        verbose=1\n",
    "                       );\n",
    "    print('***** Saving the history *****')\n",
    "\n",
    "    #PLOT HISTORY\n",
    "    mpp.save_model_history(history,os.path.join(output_dir,\"historical_\"+str(fold_var)+\".csv\"), labels=['categorical_accuracy','loss'],show=False);\n",
    "    \n",
    "    print('***** Loding the best model file ... *****')\n",
    "    # LOAD BEST MODEL to evaluate the performance of the model\n",
    "    model.load_weights(best_model_file);\n",
    "    \n",
    "    print('***** Evaluating the model ... *****')\n",
    "    results = model.evaluate(valid_data_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "    print(results,\"\\n\\n\");\n",
    "    \n",
    "    data_fold['VALIDATION_ACCURACY'].append(results['categorical_accuracy'])\n",
    "    data_fold['VALIDATION_LOSS'].append(results['loss'])\n",
    "    \n",
    "    # Data fold\n",
    "    with open(fold_status_path, 'w') as f:\n",
    "        json.dump(data_fold, f);\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    fold_var += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84facfa9-b65a-4fc8-85c7-025fd02f1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath=os.path.join(output_dir,\"final_stats.csv\");\n",
    "mean_val_acc=mpp.save_model_stat_kfold(data_fold, fpath);\n",
    "\n",
    "mpp.save_model_parameters(model, os.path.join(output_dir,'parameters_stats.txt'));\n",
    "\n",
    "print(mean_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70e376-46ee-40aa-bd19-1ea149d0a3aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fecf1b-cef9-463b-b76d-ff614de2be37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.0",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
